# QA PROMPTS

_This repository contains the code used for automatic **argument quality assessment**. It includes tools for error analysis, result visualization, and experimentation with different prompt variants._

## ğŸ“ Repository Structure

```
QA_PROMPTS/
â”‚
â”œâ”€â”€ data/
â”œâ”€â”€ error_analysis_plots/
â”œâ”€â”€ evaluation/
â”œâ”€â”€ model_responses/
â”œâ”€â”€ prompting/
â”œâ”€â”€ analyze_results_not_binary.py
â”œâ”€â”€ analyze_results.py
â”œâ”€â”€ dataset_division.py
â”œâ”€â”€ dataset.csv
â”œâ”€â”€ error_analysis.py
â”œâ”€â”€ evaluation_ft.py
â”œâ”€â”€ evaluation.py
â”œâ”€â”€ fine_tunning.py
â”œâ”€â”€ Logger.py
â”œâ”€â”€ model_1by1.py
â”œâ”€â”€ model_ft.py
â”œâ”€â”€ model.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ Usage

1. **Install dependencies**:

```bash
pip install -r requirements.txt
```
